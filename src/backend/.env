# Configuration for the LLM Backend
# Set the name of the Ollama model you want to use
LLM_MODEL_NAME=qwen3:1.7b

# Set the port for the FastAPI server
BACKEND_PORT=8000

# Set the origin(s) your Ionic app is running on during development
# For multiple origins, separate with commas (e.g., "http://localhost:8100,http://192.168.68.68:8100")
FRONTEND_ORIGINS=http://localhost:8100,http://10.0.2.2:8100,http://localhost,http://192.168.68.68:8100,capacitor://localhost